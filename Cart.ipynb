{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cart.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pablo-dev-uni/CartPole-v0-/blob/master/Cart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_5fXr7NGSnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "ee2c4255-9ad4-48c8-ce7c-6790262d927e"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from statistics import median, mean\n",
        "from collections import Counter\n",
        "\n",
        "LR = 1e-3\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "goal_steps = 500\n",
        "score_requirement = 50\n",
        "initial_games = 50000\n",
        "\n",
        "def some_random_games_first():\n",
        "    # Each of these is its own game.\n",
        "    for episode in range(5):\n",
        "        env.reset()\n",
        "        # this is each frame, up to 200...but we wont make it that far.\n",
        "        for t in range(200):\n",
        "            # This will display the environment\n",
        "            # Only display if you really want to see it.\n",
        "            # Takes much longer to display it.\n",
        "            \n",
        "            \n",
        "            # This will just create a sample action in any environment.\n",
        "            # In this environment, the action can be 0 or 1, which is left or right\n",
        "            action = env.action_space.sample()\n",
        "            \n",
        "            # this executes the environment with an action, \n",
        "            # and returns the observation of the environment, \n",
        "            # the reward, if the env is over, and other info.\n",
        "            observation, reward, done, info = env.step(action)\n",
        "            if done:\n",
        "              break\n",
        "            return done\n",
        "def initial_population():\n",
        "    # [OBS, MOVES]\n",
        "    training_data = []\n",
        "    # all scores:\n",
        "    scores = []\n",
        "    # just the scores that met our threshold:\n",
        "    accepted_scores = []\n",
        "    # iterate through however many games we want:\n",
        "    for _ in range(initial_games):\n",
        "        score = 0\n",
        "        # moves specifically from this environment:\n",
        "        game_memory = []\n",
        "        # previous observation that we saw\n",
        "        prev_observation = []\n",
        "        # for each frame in 200\n",
        "        for _ in range(goal_steps):\n",
        "            # choose random action (0 or 1)\n",
        "            action = random.randrange(0,2)\n",
        "            # do it!\n",
        "            observation, reward, done, info = env.step(action)\n",
        "            \n",
        "            # notice that the observation is returned FROM the action\n",
        "            # so we'll store the previous observation here, pairing\n",
        "            # the prev observation to the action we'll take.\n",
        "            if len(prev_observation) > 0 :\n",
        "                game_memory.append([prev_observation, action])\n",
        "            prev_observation = observation\n",
        "            score+=reward\n",
        "            if done: break\n",
        "\n",
        "        # IF our score is higher than our threshold, we'd like to save\n",
        "        # every move we made\n",
        "        # NOTE the reinforcement methodology here. \n",
        "        # all we're doing is reinforcing the score, we're not trying \n",
        "        # to influence the machine in any way as to HOW that score is \n",
        "        # reached.\n",
        "        if score >= score_requirement:\n",
        "            accepted_scores.append(score)\n",
        "            for data in game_memory:\n",
        "                # convert to one-hot (this is the output layer for our neural network)\n",
        "                if data[1] == 1:\n",
        "                    output = [0,1]\n",
        "                elif data[1] == 0:\n",
        "                    output = [1,0]\n",
        "                    \n",
        "                # saving our training data\n",
        "                training_data.append([data[0], output])\n",
        "\n",
        "        # reset env to play again\n",
        "        env.reset()\n",
        "        # save overall scores\n",
        "        scores.append(score)\n",
        "    \n",
        "    # just in case you wanted to reference later\n",
        "    training_data_save = np.array(training_data)\n",
        "    np.save('saved.npy',training_data_save)\n",
        "    \n",
        "    # some stats here, to further illustrate the neural network magic!\n",
        "    print('Average accepted score:',mean(accepted_scores))\n",
        "    print('Median score for accepted scores:',median(accepted_scores))\n",
        "    print(Counter(accepted_scores))\n",
        "    \n",
        "    return training_data\n",
        "\n",
        "  \n",
        " \n",
        "           "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0826 07:49:10.865820 140533649205120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0826 07:49:10.870924 140533649205120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0826 07:49:10.888292 140533649205120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0826 07:49:10.897432 140533649205120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W0826 07:49:10.912980 140533649205120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0826 07:49:10.914310 140533649205120 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuUSwCuVJ40b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neural_network_model(input_size):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(4,activation = 'relu',input_shape = (4,)))\n",
        "    model.add(Dense(16,activation = 'relu'))\n",
        "    model.add(Dense(4,activation = 'relu'))\n",
        "    model.add(Dense(2,activation = 'softmax'))\n",
        "    \n",
        "    model.compile(optimizer = 'adam',loss = 'categorical_crossentropy')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRH6kYVVIWMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(training_data, model=False):\n",
        "    Xt = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]))\n",
        "    yt = np.array([i[1] for i in training_data])\n",
        "\n",
        "    if not model:\n",
        "        model = neural_network_model(input_size = (len(Xt[1])))\n",
        "    \n",
        "    model.fit(Xt, yt, epochs=10, batch_size=50)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pgNnFJkIjoS",
        "colab_type": "code",
        "outputId": "7ac90ce4-c016-4edd-ace2-fcdc57237b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "training_data = initial_population()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average accepted score: 61.17097862767154\n",
            "Median score for accepted scores: 57.0\n",
            "Counter({50.0: 147, 51.0: 141, 53.0: 118, 52.0: 110, 54.0: 106, 55.0: 95, 56.0: 91, 58.0: 83, 57.0: 83, 60.0: 68, 63.0: 67, 59.0: 67, 62.0: 51, 66.0: 46, 61.0: 42, 65.0: 41, 64.0: 38, 67.0: 31, 69.0: 27, 70.0: 24, 68.0: 23, 75.0: 20, 73.0: 19, 71.0: 18, 78.0: 18, 80.0: 14, 84.0: 13, 72.0: 13, 74.0: 12, 77.0: 11, 76.0: 11, 79.0: 10, 82.0: 9, 85.0: 9, 83.0: 8, 81.0: 8, 99.0: 7, 88.0: 6, 87.0: 6, 86.0: 6, 91.0: 4, 89.0: 4, 100.0: 4, 98.0: 4, 93.0: 4, 120.0: 3, 102.0: 3, 103.0: 3, 95.0: 3, 92.0: 2, 110.0: 2, 96.0: 2, 97.0: 2, 108.0: 2, 114.0: 2, 106.0: 2, 94.0: 2, 90.0: 2, 105.0: 2, 109.0: 1, 116.0: 1, 128.0: 1, 111.0: 1, 119.0: 1, 131.0: 1, 101.0: 1, 115.0: 1, 112.0: 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co1Yjf4_KIp2",
        "colab_type": "code",
        "outputId": "1aca4bb3-3cfe-4b66-d5c6-6b67569fd46c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model = train_model(training_data)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "106984/106984 [==============================] - 3s 26us/step - loss: 0.6705\n",
            "Epoch 2/10\n",
            "106984/106984 [==============================] - 3s 24us/step - loss: 0.6619\n",
            "Epoch 3/10\n",
            "106984/106984 [==============================] - 2s 23us/step - loss: 0.6610\n",
            "Epoch 4/10\n",
            "106984/106984 [==============================] - 2s 23us/step - loss: 0.6604\n",
            "Epoch 5/10\n",
            "106984/106984 [==============================] - 2s 23us/step - loss: 0.6603\n",
            "Epoch 6/10\n",
            "106984/106984 [==============================] - 3s 24us/step - loss: 0.6600\n",
            "Epoch 7/10\n",
            "106984/106984 [==============================] - 2s 23us/step - loss: 0.6600\n",
            "Epoch 8/10\n",
            "106984/106984 [==============================] - 2s 23us/step - loss: 0.6599\n",
            "Epoch 9/10\n",
            "106984/106984 [==============================] - 2s 23us/step - loss: 0.6597\n",
            "Epoch 10/10\n",
            "106984/106984 [==============================] - 2s 22us/step - loss: 0.6596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q4SHEUT_KAf",
        "colab_type": "code",
        "outputId": "69cfa97d-0f0c-4b43-896b-6e3def7cdaba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "scores = []\n",
        "choices = []\n",
        "for each_game in range(10):\n",
        "    score = 0\n",
        "    game_memory = []\n",
        "    prev_obs = []\n",
        "    env.reset()\n",
        "    for _ in range(goal_steps):\n",
        "      if len(prev_obs)==0:\n",
        "        action = random.randrange(0,2)\n",
        "      else:\n",
        "        action = np.argmax(model.predict(prev_obs.reshape(1,len(prev_obs)))[0])\n",
        "\n",
        "      choices.append(action)\n",
        "                \n",
        "      new_observation, reward, done, info = env.step(action)\n",
        "      prev_obs = new_observation\n",
        "      game_memory.append([new_observation, action])\n",
        "      score+=reward\n",
        "      if done: break\n",
        "\n",
        "    scores.append(score)\n",
        "\n",
        "print('Average Score:',sum(scores)/len(scores))\n",
        "print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
        "print(score_requirement)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Score: 192.0\n",
            "choice 1:0.5010416666666667  choice 0:0.49895833333333334\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7R5uTTScbGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}